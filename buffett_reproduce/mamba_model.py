















# class SelfAttentionModel(nn.Module):
#     def __init__(
#         self,  
#         src_dim=512,
#         embed_dim=768, 
#         num_heads=8,
#     ):
#         super(SelfAttentionModel, self).__init__()
#         self.query_dim
#         self.embed_dim = embed_dim
#         self.query_proj = nn.Linear(embed_dim, src_dim)
#         self.multihead_attn = nn.MultiheadAttention(src_dim, num_heads)
        
        
#     def forward(self, src, query):
        
#         return src










