{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buffett/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from enrollment_model import MyModel\n",
    "# from load_data import BEATS_path, ORIG_mixture, ORIG_target #, stems\n",
    "# from dataset import MusicDataset\n",
    "from loss import L1SNR_Recons_Loss, L1SNRDecibelMatchLoss\n",
    "from utils import _load_config\n",
    "from metrics import (\n",
    "    AverageMeter, cal_metrics, safe_signal_noise_ratio, MetricHandler\n",
    ")\n",
    "\n",
    "from models.types import InputType, OperationMode, SimpleishNamespace\n",
    "from data.moisesdb.datamodule import (\n",
    "    MoisesTestDataModule,\n",
    "    MoisesValidationDataModule,\n",
    "    MoisesDataModule,\n",
    "    MoisesBalancedTrainDataModule,\n",
    "    MoisesVDBODataModule,\n",
    ")\n",
    "\n",
    "\n",
    "from dismix.dismix_model import DisMixModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cropping\n",
    "# def crop_sustain_phase(mel_spectrogram, crop_frames=10, start_frame=None):\n",
    "#     \"\"\"\n",
    "#     Crop a 320ms segment (10 frames) from the sustain phase of the mel spectrogram.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - mel_spectrogram: Mel spectrogram to crop.\n",
    "#     - crop_frames: Number of frames to crop (10 frames corresponds to 320ms).\n",
    "#     - start_frame: Starting frame for cropping (if None, find from sustain phase).\n",
    "    \n",
    "#     Returns:\n",
    "#     - Cropped mel spectrogram segment, start_frame used for alignment.\n",
    "#     \"\"\"\n",
    "#     # Calculate energy for each frame\n",
    "#     frame_energy = torch.sum(mel_spectrogram, dim=0)\n",
    "    \n",
    "#     # Find the maximum energy frame index (attack phase) if start_frame is not provided\n",
    "#     if start_frame is None:\n",
    "#         max_energy_frame = torch.argmax(frame_energy)\n",
    "#         # Define the starting frame of the sustain phase, a few frames after the peak energy\n",
    "#         start_frame = max_energy_frame + 5  # Shift 5 frames after peak to avoid attack phase\n",
    "    \n",
    "#     # Ensure the crop window does not exceed the spectrogram length\n",
    "#     if start_frame + crop_frames > mel_spectrogram.size(1):\n",
    "#         start_frame = max(0, mel_spectrogram.size(1) - crop_frames)\n",
    "    \n",
    "#     # Crop the mel spectrogram segment\n",
    "#     cropped_segment = mel_spectrogram[:, start_frame:start_frame + crop_frames]\n",
    "    \n",
    "#     return cropped_segment, start_frame\n",
    "\n",
    "\n",
    "\n",
    "# def processing(mel_spectrogram, start_frame=None):\n",
    "    \n",
    "#     # Convert complex-valued spectrogram to magnitude (real values)\n",
    "#     mel_spectrogram_magnitude = torch.abs(mel_spectrogram)\n",
    "    \n",
    "#     # Convert amplitude to decibel scale\n",
    "#     mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB(top_db=80)(mel_spectrogram_magnitude)\n",
    "    \n",
    "#     # Crop a 320ms segment (10 frames) from the sustain phase\n",
    "#     cropped_mel_spectrogram, start_frame = crop_sustain_phase(mel_spectrogram_db.squeeze(0), crop_frames=10, start_frame=start_frame)\n",
    "    \n",
    "#     return cropped_mel_spectrogram, start_frame\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:2\n",
      "Training with stems:  ['bass', 'drums', 'lead_male_singer', 'lead_female_singer', 'distorted_electric_guitar', 'clean_electric_guitar', 'acoustic_guitar', 'grand_piano', 'electric_piano']\n",
      "Loading query tuples from /home/buffett/NAS_NTU/moisesdb/queries/4/chunk294400-hop264600/query441000-n1/queries-9stems:b:d:l:l:d:c:a:g:e.csv\n",
      "Loading query tuples from /home/buffett/NAS_NTU/moisesdb/queries/5/chunk294400-hop264600/query441000-n1/queries-9stems:b:d:l:l:d:c:a:g:e.csv\n"
     ]
    }
   ],
   "source": [
    "# Init settings\n",
    "wandb_use = False # False\n",
    "lr = 1e-3 # 1e-4\n",
    "num_epochs = 1 # 500\n",
    "batch_size = 4 # 8\n",
    "n_srcs = 1\n",
    "emb_dim = 768 # For BEATs\n",
    "query_size = 512 # 512\n",
    "mix_query_mode = \"Hyper_FiLM\" # \"Transformer\"\n",
    "q_enc = \"Passt\"\n",
    "config_path = \"config/train.yml\"\n",
    "mask_type = \"L1\"\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Training on device:\", device)\n",
    "\n",
    "\n",
    "def to_device(batch, device=device):\n",
    "    batch.mixture.audio = batch.mixture.audio.to(device) # torch.Size([BS, 2, 294400])\n",
    "    batch.sources.target.audio = batch.sources.target.audio.to(device) # torch.Size([BS, 2, 294400])\n",
    "    batch.query.audio = batch.query.audio.to(device) # torch.Size([BS, 2, 441000])\n",
    "    return batch\n",
    "\n",
    "\n",
    "if wandb_use:\n",
    "    wandb.init(\n",
    "        project=\"Query_ss\",\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"Transformer_UNet Using 9 stems\",\n",
    "            \"dataset\": \"MoisesDB\",\n",
    "            \"epochs\": num_epochs,\n",
    "        },\n",
    "        notes=f\"{mix_query_mode} + {mask_type} Loss + 512 query size\",\n",
    "    )\n",
    "\n",
    "\n",
    "config = _load_config(config_path)\n",
    "stems = config.data.train_kwargs.allowed_stems\n",
    "print(\"Training with stems: \", stems)\n",
    "\n",
    "datamodule = MoisesDataModule(\n",
    "    data_root=config.data.data_root,\n",
    "    batch_size=batch_size, #config.data.batch_size,\n",
    "    num_workers=config.data.num_workers,\n",
    "    train_kwargs=config.data.get(\"train_kwargs\", None),\n",
    "    val_kwargs=config.data.get(\"val_kwargs\", None),\n",
    "    test_kwargs=config.data.get(\"test_kwargs\", None), # Cannot use now\n",
    "    datamodule_kwargs=config.data.get(\"datamodule_kwargs\", None),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the enrollment model\n",
    "model = DisMixModel(\n",
    "    input_dim=128, \n",
    "    latent_dim=64, \n",
    "    hidden_dim=256, \n",
    "    gru_hidden_dim=256,\n",
    "    num_frames=10,\n",
    "    pitch_classes=52,\n",
    "    output_dim=128,    \n",
    ").to(device)\n",
    "\n",
    "\n",
    "def window_fn(win_length):\n",
    "    return torch.hann_window(win_length).to(device)\n",
    "\n",
    "# Initialize the Spectrogram transform with the correct window function\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=2048,\n",
    "    win_length=2048,\n",
    "    hop_length=512,\n",
    "    pad_mode=\"constant\",\n",
    "    pad=0,\n",
    "    window_fn=window_fn,  # Pass the callable window function\n",
    "    wkwargs=None,\n",
    "    power=None,\n",
    "    normalized=True,\n",
    "    center=True,\n",
    "    onesided=True,\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8192 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 294400]) torch.Size([1, 2, 441000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(datamodule.train_dataloader()):\n",
    "    batch = InputType.from_dict(batch)\n",
    "    batch = to_device(batch)\n",
    "    print(batch.mixture.audio.shape, batch.query.audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1025, 862]) torch.Size([1, 2, 1025, 576])\n",
      "torch.Size([2, 10, 862]) torch.Size([2, 10, 576])\n"
     ]
    }
   ],
   "source": [
    "batch = InputType.from_dict(batch)\n",
    "batch = to_device(batch)\n",
    "\n",
    "batch.mixture.spectrogram = stft(batch.mixture.audio)\n",
    "batch.query.spectrogram = stft(batch.query.audio)\n",
    "\n",
    "mixture_orig = batch.mixture.spectrogram.to(device)\n",
    "query_orig = batch.query.spectrogram.to(device)\n",
    "print( query_orig.shape, mixture_orig.shape)\n",
    "\n",
    "\n",
    "# Processing\n",
    "query, start_frame = processing(query_orig)\n",
    "mixture, _ = processing(mixture_orig, start_frame)\n",
    "print(query.shape, mixture.shape)\n",
    "\n",
    "# rec_mixture, pitch_latent, pitch_logits, timbre_latent, timbre_mean, timbre_logvar, eq = model(mixture, query)\n",
    "\n",
    "# print(pitch_latent.shape, timbre_latent.shape)\n",
    "\n",
    "# pitch_data.append(pitch_latent)\n",
    "# timbre_data.append(timbre_latent)\n",
    "# stem_data.append(batch.metadata.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 48/2048 [12:26<5:30:47,  9.92s/it] "
     ]
    }
   ],
   "source": [
    "spec_store = []\n",
    "stem_names = []\n",
    "song_id = []\n",
    "\n",
    "# Training loop\n",
    "for batch in tqdm(datamodule.train_dataloader()):\n",
    "    batch = InputType.from_dict(batch)\n",
    "    batch = to_device(batch)\n",
    "    \n",
    "    # batch.mixture.spectrogram = stft(batch.mixture.audio)\n",
    "    batch.query.spectrogram = stft(batch.query.audio)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        spec_store.append(batch.query.spectrogram[i])\n",
    "        stem_names.append(batch.metadata.stem[i])\n",
    "        song_id.append(batch.metadata.query.song_id[i])\n",
    "\n",
    "    # break\n",
    "    # mixture_orig = batch.mixture.spectrogram.to(device)\n",
    "    # query_orig = batch.query.spectrogram.to(device)\n",
    "    \n",
    "    \n",
    "    # # Processing\n",
    "    # query, start_frame = processing(query_orig)\n",
    "    # mixture, _ = processing(mixture_orig, start_frame)\n",
    "    # print(query.shape, mixture.shape)\n",
    "    \n",
    "    # rec_mixture, pitch_latent, pitch_logits, timbre_latent, timbre_mean, timbre_logvar, eq = model(mixture, query)\n",
    "    \n",
    "    # print(pitch_latent.shape, timbre_latent.shape)\n",
    "    \n",
    "    # pitch_data.append(pitch_latent)\n",
    "    # timbre_data.append(timbre_latent)\n",
    "    # stem_data.append(batch.metadata.stem)\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 1025, 862])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.query.spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
